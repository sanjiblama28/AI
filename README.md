# AI-weekly activities

<br/>

>**Name: Sanjib Tamang**<br/>
>**ID: 12194939**<br/>
>**Course: AI Application System**<br/>
>**Professor: Mehdi Pirahandeh**<br/>

<br/>

### [Week-2] <br/>
• Get Started with Google Colab<br/>
• Uploading data<br/>
• Importing Kaggle’s dataset<br/>
• Basic File Operations<br/>
<br/>

### [Week-3](Week3_939.ipynb): <br/>
**The backpropagation algorithm:<br/>**
• The forward pass<br/>
• The backward pass<br/>
<br/>

### [Week-4](Week4_939.ipynb): <br/>
**Fully Connected Networks Applied to Multiclass Classification:**<br/>
• Introduction to Datasets used when training networks<br/>
• Extending the network and learning algorithm to do multiclass classification<br/>
• Network for Digit Classification<br/>
• Loss Function for Multiclass Classification<br/>
• Programming Example: Classifying handwritten Digits<br/>
• Mini-Batch gradient descent<br/>

<br/>

### [Week-5](Week5_939.ipynb) | [Week-5(2)](Week5(2)_939.ipynb): <br/>
• What is Tensorflow<br/>
• Computational graph<br/>
• Variables, Constants and Placeholders in TensorFlow<br/>
• Tensorboard visualization<br/>
• tf.summary.scalar command<br/>
• tf.summary.histogram command<br/>

<br/>

### [Week-6](week6 (1)_939.ipynb) | [Week-6(2)](week6_2-939.ipynb): <br/>
• Linear Regression using TensorFlow<br/>
• Visualization of Linear Regression parameters using TensorFlow<br/>
• Digit Classification | Neural network to classify MNIST dataset using TensorFlow<br/>

<br/>

### [Week-7](Week 7 (1) -939.ipynb) | [Week-7(2)](week7(2)_939.ipynb): <br/>
• Convolutional Neural Networks<br/>
• The CIFAR-10 Dataset<br/>
• Characteristics and building blocks for convolutionallayers<br/>
• Combining feature maps into a convolutional layer<br/>
• Combining convolutional and fully connected layers into anetwork<br/>
• Effects of sparse connections and weight sharing<br/>
• Image classification with a convolutional network<br/>
<br/>

### [Week-9](Week 9(1-2).ipynb) | [Week-9(2)](Week_9_2(1-2).ipynb): <br/>
• Logistic unit for binary classification<br/>
• Softmax unit for multiclass classification<br/>
• Linear unit for regression<br/>
• The Boston Housing dataset<br/>
• Predicting house prices with a DNN<br/>
• Improving generalization with regularization<br/>
• Experiment: Deeper and regularized models for house price prediction<br/>
• Concluding remarks on output units and regression problems<br/>
<br/>

### [Week-10](week_10(1-2).ipynb) | [Week-10(2)](Week10(2)_939.ipynb): <br/>
#### Deeper CNNs and Pretrained Models <br/>
• VGGNet<br/>
• GoogLeNet<br/>
• ResNet<br/>
• Transfer Learning<br/>
• Data Augmentation as a Regularization Technique<br/>
• Mistakes made by CNNs<br/>
• Reducing parameters with Depthwise Separable Convolution<br/>
• Striking the right network design balance with EfficientNet<br/>
<br/>


### [Week-11](week11_Lab(1-4).ipynb)
• Limitations of Feedforward Networks<br/>
• Recurrent Neural Networks<br/>
• Mathematical Representation of a Recurrent layer<br/>
• Combining layers into an RNN<br/>
• Alternative veiw of RNN and Unrolling in Time<br/>
• Backpropagation Through Time<br/>
• Programming Example: Forecasting book sales<br/>
<br/>

### [Week-12](Week12_939.ipynb)
• Keeping Gradients Healthy<br/>
• Introduction to LSTM<br/>
• Creating a network of LSTM cells<br/>
• Alternative view of LSTM<br/>
<br/>

### [Week-13](week13(1)_939.ipynb) | [Week-13(2)](Week13(2)_939.ipynb): <br/>
#### Natural Language Processing, binary unigram, binary bigram, TF-IDF bigram model<br/>
#### A sequence model built on one-hot encoded vector sequences<br/>
• Encoding text<br/>
• Longer-term prediction and autoregressive models<br/>
• Beam Search<br/>
• Bidirectional RNNS<br/>
• Different combinations of input and output sequences<br/>
<br/>

### [Week-14](Week14_939.ipynb)

#### Natural Language Processing using transformer encoder

